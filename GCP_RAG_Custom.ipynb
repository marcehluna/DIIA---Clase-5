{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "aTmjxcqCAjc7",
        "UF6A2H3HAg3i",
        "VXMaIFfOUne5",
        "roye3gRf9G3b"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcehluna/DIIA---Clase-5/blob/main/GCP_RAG_Custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom RAG con GCP\n",
        "\n",
        "## Introducción y contexto\n",
        "A lo largo de las sesiones anteriores, hemos explorado en profundidad los sistemas de Generación Aumentada por Recuperación (RAG), una arquitectura fundamental en el campo del Procesamiento del Lenguaje Natural (PLN) que permite a los modelos de lenguaje de gran tamaño (LLMs) generar respuestas más precisas y contextualizadas. Hemos aprendido a implementar y a interactuar con sistemas RAG utilizando servicios mayoritariamente autogestionados dentro del ecosistema de Google Cloud.\n",
        "\n",
        "Hasta ahora, nuestro enfoque se ha centrado en utilizar componentes pre-construidos donde la fase de indexación —el proceso de ingestar y preparar los datos para que el sistema de recuperación pueda actuar sobre ellos— se realizaba de forma casi automática. Hemos tratado con servicios totalmente gestionados y administrados, lo que nos ha permitido concentrarnos en la interacción entre el recuperador y el generador.\n",
        "\n",
        "Sin embargo, el mundo real necesita de soluciones más detalladas y precisas. La vasta mayoría de la información valiosa para las empresas y organizaciones se encuentra \"atrapada\" en formatos complejos como los documentos PDF. Estos archivos no son solo contenedores de texto; son un lienzo que combina texto, imágenes, tablas, columnas y una estructura visual que es trivial para un humano, pero inmensamente compleja para una máquina.\n",
        "Este cuaderno marca el siguiente paso en nuestra formación como especialistas en Deep Learning: nos adentraremos en el desafío de construir nosotros mismos el pipeline de indexación."
      ],
      "metadata": {
        "id": "n8LuCbjiL6Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descripción del problema\n",
        "\n",
        "Nuestro objetivo es procesar un documento PDF desde cero, extraer su contenido de manera inteligente y prepararlo para ser vectorizado e insertado en una base de datos vectorial. Este proceso es la piedra angular de cualquier sistema RAG robusto y escalable.\n",
        "\n",
        "Para lograrlo, no nos basta con una simple extracción de texto. Necesitamos entender la estructura del documento. Aquí es donde entran en juego dos tecnologías críticas:\n",
        "\n",
        "* Análisis de Disposición de Página (Layout Analysis): Esta parte del procesado nos especifica dónde y de qué tipo es el contenido de la página. Identifica párrafos, títulos, encabezados, pies de página, tablas y columnas. Entender el layout es crucial porque el orden y la agrupación del texto definen su contexto semántico. Por ejemplo, un texto en una columna debe leerse de arriba abajo antes de pasar a la siguiente columna, y los datos dentro de una tabla tienen una relación entre sí que se perdería con una lectura lineal.\n",
        "\n",
        "* Reconocimiento Óptico de Caracteres (OCR): Es el proceso de convertir imágenes de texto (como las que se encuentran en un PDF escaneado) en texto real que una máquina puede leer y procesar. Es la tarea siguiente a realizar una vez el _layout analysis_ ha hecho su parte.\n",
        "\n",
        "  ![](https://miro.medium.com/v2/resize:fit:1400/0*9WSDnnY0aI05R_cq.png)\n",
        "\n",
        "Para esta tarea, utilizaremos el servicio `PPStructureV3` de `PaddlePaddle`. Este no es un servicio de OCR convencional; está diseñado específicamente para el análisis inteligente de documentos y ofrece capacidades de vanguardia:\n",
        "\n",
        "1. Modelos Pre-entrenados: Utiliza modelos de Deep Learning entrenados en miles de millones de documentos, capaces de entender una variedad de layouts complejos sin necesidad de entrenamiento adicional.\n",
        "\n",
        "2. Identificación de Bloques: El servicio no devuelve una masa de texto desordenada. En su lugar, segmenta el documento en una estructura jerárquica: Páginas -> Bloques -> Párrafos -> Palabras -> Símbolos. Cada uno de estos elementos viene con su contenido de texto y sus coordenadas (bounding box).\n",
        "\n",
        "3. Manejo de Entidades: Es capaz de identificar entidades específicas como tablas, listas y otros elementos estructurales, permitiendo una extracción de datos mucho más granular y precisa.\n",
        "\n",
        "Al utilizar `PaddlePaddle`, podremos descomponer nuestro PDF en fragmentos de texto lógicos y coherentes. Estos fragmentos (o chunks) serán las unidades que posteriormente convertiremos en vectores para alimentar nuestra base de datos vectorial, asegurando que cada vector represente una idea o un concepto semánticamente cohesivo.\n",
        "\n",
        "A continuación, procederemos con la implementación práctica, pero previo a ellos es importante asegurarse de que tenemos habilitada la GPU en Colab.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJsR6G-TPHbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 0: Instalación de librerías"
      ],
      "metadata": {
        "id": "aTmjxcqCAjc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QixyaOwYg8_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash\n",
        "# Install parallel\n",
        "sudo apt update && sudo apt install ghostscript -y && sudo apt install parallel -y\n",
        "\n",
        "# Install uv\n",
        "pip install uv\n",
        "\n",
        "# Install PaddlePaddle GPU 3.0.0 (for CUDA 12.6)\n",
        "uv pip install --pre paddlepaddle-gpu -i https://www.paddlepaddle.org.cn/packages/nightly/cu126/\n",
        "\n",
        "# Install PaddleOCR version that includes PPStructureV3\n",
        "uv pip install paddleocr==3.1.0 sentence-transformers einops timm pillow qdrant-client"
      ],
      "metadata": {
        "id": "iSh9UFvbKpF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: Lectura del PDF\n",
        "\n",
        "En primer lugar, vamos a definir un método que nos permita, a partir de una determinada ruta en la que especifiquemos un archivo `.pdf`, dividirlo en páginas independientes, y convertir cada una de ellas a `.jpg`."
      ],
      "metadata": {
        "id": "UF6A2H3HAg3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import logging as log\n",
        "from typing import List, Dict\n",
        "\n",
        "log.basicConfig(level=log.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def convert_pdf_to_images(pdf_path: str, output_dir: str = \"images\", dpi: int = 300) -> Dict[str, List[str]]:\n",
        "\n",
        "    filepaths = []\n",
        "    if os.path.isfile(pdf_path) and pdf_path.lower().endswith(\".pdf\"):\n",
        "        filepaths.append(pdf_path)\n",
        "    elif os.path.isdir(pdf_path):\n",
        "        for root, _, filenames in os.walk(pdf_path):\n",
        "            for filename in filenames:\n",
        "                if filename.lower().endswith(\".pdf\"):\n",
        "                    filepaths.append(os.path.join(root, filename))\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No valid PDF files found at the specified path: {pdf_path}\")\n",
        "\n",
        "    if not filepaths:\n",
        "        raise FileNotFoundError(f\"No PDF files were found to process in {pdf_path}.\")\n",
        "\n",
        "    file2image = {}\n",
        "    for filepath in filepaths:\n",
        "        name = os.path.splitext(os.path.basename(filepath))[0]\n",
        "        output_path = os.path.join(output_dir, name)\n",
        "\n",
        "        if os.path.isdir(output_path) and os.listdir(output_path):\n",
        "            log.info(f\"Images for '{name}' already exist. Skipping conversion.\")\n",
        "            file2image[name] = sorted([os.path.join(output_path, img) for img in os.listdir(output_path) if img.endswith(\".jpg\")])\n",
        "            continue\n",
        "\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        log.info(f\"Converting '{name}' to images at {dpi} DPI...\")\n",
        "        cmd = [\n",
        "            \"gs\", \"-dNOPAUSE\", \"-dBATCH\", \"-sDEVICE=jpeg\",\n",
        "            f\"-r{dpi}\", f\"-sOutputFile='{os.path.join(output_path, 'page_%03d.jpg')}'\",\n",
        "            f\"'{filepath}'\",\n",
        "        ]\n",
        "        try:\n",
        "            subprocess.run(\" \".join(cmd), check=True, capture_output=True, text=True, shell=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            log.error(f\"Ghostscript failed for {filepath}: {e.stderr}\")\n",
        "            raise RuntimeError(f\"PDF to image conversion failed for {filepath}: {e}\")\n",
        "\n",
        "        file2image[name] = sorted([os.path.join(output_path, img) for img in os.listdir(output_path) if img.endswith(\".jpg\")])\n",
        "    return file2image"
      ],
      "metadata": {
        "id": "gRGjFbR0Bnil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos un `.pdf` como referencia:"
      ],
      "metadata": {
        "id": "c_W-rC4tjee0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import urllib.parse\n",
        "from google.colab import files\n",
        "\n",
        "# Upload a PDF file from your local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename of the uploaded file\n",
        "for fn, content in uploaded.items():\n",
        "    # Save the uploaded file\n",
        "    with open(fn, 'wb') as f:\n",
        "        f.write(content)\n",
        "    print(f\"Successfully uploaded and saved as '{fn}'\")\n",
        "\n",
        "# The variable 'fn' will now hold the name of the uploaded file.\n",
        "# You can proceed with the rest of your notebook using this filename."
      ],
      "metadata": {
        "id": "mbUnFOQhjh5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez definido el método, nos aseguramos de que generamos un directorio donde las imágenes puedan ser almacenadas, y lo empleamos:"
      ],
      "metadata": {
        "id": "QUEBwLi_GQhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Especificamos la ruta donde se aloja el PDF\n",
        "pdf_path = f\"{fn}\"\n",
        "# Nos aseguramos de que generamos un directorio para las imágenes\n",
        "output_dir = 'images'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Ejecutamos el método construído\n",
        "doc2images = convert_pdf_to_images(pdf_path, output_dir)"
      ],
      "metadata": {
        "id": "ynbUfJ4NGQK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Procesamiento `OCR`\n",
        "\n",
        "Como ya indicamos, `PaddlePaddle` dispone de sistemas de alto rendimiento para nuestro propósito, incluyendo complejos _pipelines_ que distinguen entre tipos de objetos, posicionamiento, idiomas, entre otros:\n",
        "\n",
        "![](https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/PP-StructureV3/algorithm_ppstructurev3.png)\n",
        "\n",
        "Dado que, además, es un servicio de código abierto, podemos emplearlo cómodamente en nuestras soluciones. A continuación, mostramos cómo hacerlo:"
      ],
      "metadata": {
        "id": "VXMaIFfOUne5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from paddleocr import PPStructureV3\n",
        "import os\n",
        "\n",
        "pipeline = PPStructureV3(\n",
        "    layout_detection_model_name=\"PP-DocLayout_plus-L\",\n",
        "    text_recognition_model_name=\"PP-OCRv5_server_rec\",\n",
        "    use_doc_orientation_classify=True,\n",
        "    use_doc_unwarping=True,\n",
        "    use_textline_orientation=True,\n",
        "    device=\"gpu\",\n",
        ")\n",
        "\n",
        "output = pipeline.predict(doc2images[os.path.splitext(fn)[0]])"
      ],
      "metadata": {
        "id": "VILUDbceocXF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Análisis del resultado e indexación\n",
        "\n",
        "El resultado de este análisis es tan detallado como el proceso en sí. En concreto, la salida del `pipeline` será una lista de diccionarios, en la que nos interesa particularmente el elemento `parsing_res_list`. En él, tendremos toda la información disponible; en particular, dicha información estará tipificada en los siguientes conjuntos:\n",
        "\n",
        "* header\n",
        "* doc_title\n",
        "* text\n",
        "* paragraph_title\n",
        "* figure_title\n",
        "* image\n",
        "* footer\n",
        "* content\n",
        "* number\n",
        "* table\n",
        "* footnote\n",
        "* chart\n",
        "\n"
      ],
      "metadata": {
        "id": "IMJSAUWjWjkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A fin de encontrar un buen equilibrio entre eficacia y eficiencia, analizaremos los contenidos más esenciales, que son `text`, `content`, `paragraph_title`, `image`, `table` y `chart`. Teniendo en consideración que nuestro objetivo final es vectorizar nuestro documento (previo paso de indexación o _chunking_), la estrategia que tomamos es:\n",
        "\n",
        "1. Para los elementos de texto (que incluye a `text`, `content` y `paragraph_title`), simplemente tomamos el contenido y lo vectorizamos. Un ejemplo de lo que el proceso `OCR` nos devuelve sobre estos elementos es:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    'label': 'text',\n",
        "    'order_label': 'normal_text',\n",
        "    'bbox': [166, 952, 1633, 1192],\n",
        "    'content': 'An Architecture for Building Agentic Applications in the Enterprise ',\n",
        "    'width': np.float32(1466.4224),\n",
        "    'height': np.float32(239.9646),\n",
        "    'area': 351889.4552630186,\n",
        "    'num_of_lines': 2,\n",
        "    'image': None,\n",
        "    'index': 0,\n",
        "    'order_index': 2,\n",
        "    'text_line_width': np.float64(1328.0),\n",
        "    'text_line_height': np.float64(121.5),\n",
        "    'child_blocks': [],\n",
        "    'direction': 'horizontal',\n",
        "    'secondary_direction': 'vertical',\n",
        "    #...\n",
        "  }\n",
        "  ```\n",
        "2. Para elementos de tipo tabla, tendríamos la opción de vectorizar el texto resultado de nuestro sistema OCR, o bien vectorizar directamente la tabla como una imagen. En este caso, tomaremos la priemra aproximación al problema por cuestiones de eficiencia, aunque para tablas muy complejas podremos siempre optar por la segunda. Nuestro proceso nos retorna de estos objetos lo siguiente:\n",
        "\n",
        "  ```json\n",
        "    {\n",
        "      'label': 'table',\n",
        "      'order_label': 'vision',\n",
        "      'bbox': [297, 497, 1498, 1953],\n",
        "      'content': '<html><body><table><tbody><tr><td>Developer or Provider</     td><td>Model or Product</td><td>Release Date</td><td>Description</td></     tr><tr><td>OpenAl</td><td>GPT-3</td><td>May 2020</  td><td>175billionparameter   LLMwith2048token contextwindow</td></  tr><tr><td>OpenAl</td><td>ChatGPT</   td><td>November 2022</td><td>Consumer   chatbotapplication,poweredby GPT-3.   5Turbo</td></tr><tr><td>Microsoft   Azure</td><td>OpenAl Service</ td><td>January  2023</td><td>Managed   serviceofferingLLMs fromOpenAI</td></ tr><tr><td>Amazon Web  Services</ td><td>Bedrock</td><td>September 2023</ td><td>Managed serviceoffering   LLMs from various developers</td></  tr><tr><td>Dataiku</td><td>LLMMesh</    td><td>September 2023</ td><td>CommercialLLMMeshoffering forconnecting to   LLMs   and buildinq agentic  applications in the enterprise</td></    tr><tr><td>Databricks</td><td>DBRX</  td><td>March 2024</ td><td>Open-weiqhts  mixture ofexperts model with 132B   total parameters  and 32k-token input context   window,licensed forcommercial  use</td></  tr><tr><td>Meta</td><td>LLaMA3 (8B,70B) </td><td>April2024</  td><td>UpdatedLLMwith4096-tokeninputcontext window,  withupdated     licenseallowingcertain commercial uses</td></tr><tr><td>Mistral</     td><td>Mixtral 8x22B</td><td>April 2024</td><td>Open-weights mixture    ofexperts   model with up to141Bparameters and64k-inputcontext window,  licensed  forcommercial use</td></tr><tr><td>OpenAl</td><td>GPT-40</   td><td>May 2024</  td><td>Multimodal LLM supporting voice-to-voice  generation  and128k-token input  context window</td></tr><tr><td>OpenAl</  td><td>01</ td><td>September 2024</ td><td>Reasoningmodel withbuilt-in  chain-of-thought  for complex scientificand  mathematical problems</td></  tr><tr><td>DeepSeek</ td><td>R1</td><td>January   2025</td><td>Open-source  reasoning model (MIT  license) optimized for math,  coding, and logic</td></ tr></tbody></table></  body></html>',\n",
        "      'width': np.float32(1200.5344),\n",
        "      'height': np.float32(1456.0574),\n",
        "      'area': 1748046.9994115233,\n",
        "      'num_of_lines': 1,\n",
        "      'image': {'path': 'imgs/img_in_table_box_297_497_1498_1953.jpg',\n",
        "       'img': <PIL.Image.Image image mode=RGB size=1201x1456>},\n",
        "      'index': 0,\n",
        "      'child_blocks': [],\n",
        "      'direction': 'horizontal',\n",
        "      'secondary_direction': 'vertical',\n",
        "      #...\n",
        "    }\n",
        "  ```\n",
        "\n",
        "3. Para elementos tipo imagen (entre los que están `image` y `chart`), directamente accederemos al atributo `image` para tomar la imagen como _array_ numérico:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "  'label': 'image',\n",
        "   'order_label': 'vision',\n",
        "   'bbox': [121, 1709, 1676, 2439],\n",
        "   'content': 'The Universal Al Platform T \\nControl Agents Enterprise  Orchestration Continuous Optimization Central Govermance Multi-Agents \\nAgent  Observabillity Strategic Oversight Multi-Models \\nAnalytics Models   \\nSecurity & Guardrails Agent Performance Risk Monitoring \\nCreate Agents   Knowledge Worker Developer Data & Al Infrastructure Third-Party Agents & Tools  A aws W ',\n",
        "   'seg_start_coordinate': np.float64(694.0),\n",
        "   'seg_end_coordinate': np.float64(1548.0),\n",
        "   'width': np.float32(1554.9723),\n",
        "   'height': np.float32(729.7499),\n",
        "   'area': 1134740.8388400525,\n",
        "   'num_of_lines': 1,\n",
        "   'image': {'path': 'imgs/img_in_image_box_121_1709_1676_2439.jpg',\n",
        "    'img': <PIL.Image.Image image mode=RGB size=1555x730>},\n",
        "   'index': 3,\n",
        "   'order_index': None,\n",
        "   'text_line_width': np.float64(614.4166666666666),\n",
        "   'text_line_height': np.float64(34.25),\n",
        "   'child_blocks': [],\n",
        "   'direction': 'horizontal',\n",
        "   'secondary_direction': 'vertical',\n",
        "   #...\n",
        "   }\n",
        "  ```\n",
        "\n",
        "  Observamos en particular que el proceso `OCR` hace un esfuerzo por proporcionar información textual sobre imágenes, gráficas y tablas.\n",
        "\n",
        "Vamos a proceder ahora a almacenar la información que estrictamente necesitamos del proceso `OCR`. Concretamente, para hacer una indexación completa, necesitaremos la información que mostramos en la siguiente estructura de datos:"
      ],
      "metadata": {
        "id": "GD6V1JmeFiLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencias\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
        "from PIL import Image\n",
        "from typing import List, Union, Dict, Any, Optional\n",
        "import numpy as np\n",
        "\n",
        "# Definimos una estructura que contenga nuestros elementos atómicos\n",
        "class DocumentElement(BaseModel):\n",
        "    \"\"\"\n",
        "    Representa un único elemento atómico detectado en un documento, como un\n",
        "    bloque de texto, un título, una tabla o una imagen.\n",
        "\n",
        "    Esta clase utiliza Pydantic para validar los datos y asegurar que cada\n",
        "    elemento tenga la estructura correcta.\n",
        "    \"\"\"\n",
        "\n",
        "    document_id: str          # Identificador del documento al que pertenece.\n",
        "    page_number: int          # Número de la página donde se encuentra el elemento.\n",
        "    layout_index: int         # Índice del elemento dentro del análisis del layout.\n",
        "    label: str                # Etiqueta específica del modelo (ej: 'text', 'title', 'table').\n",
        "    region_label: str         # Etiqueta de la región a la que pertenece (ej: 'header', 'body').\n",
        "\n",
        "    # El cuadro delimitador (bounding box) [x1, y1, x2, y2].\n",
        "    # Field(...) indica que es obligatorio. min_items y max_items aseguran que siempre tenga 4 coordenadas.\n",
        "    bbox: List[int] = Field(..., min_items=4, max_items=4)\n",
        "\n",
        "    # El contenido del elemento. Puede ser texto (str) o una imagen (np.array).\n",
        "    content: Union[str, List[float], List[List[float]], List[List[List[float]]]]"
      ],
      "metadata": {
        "id": "mUH_izE6ThsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almacenamos ahora en una lista los resultados:"
      ],
      "metadata": {
        "id": "xLft8w05oM_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una lista vacía donde iremos acumulando los `chunks`\n",
        "l = []\n",
        "\n",
        "# Iteramos sobre cada página\n",
        "for page_number, item in enumerate(output):\n",
        "    # Iteramos sobre cada elemento de salida\n",
        "    for elem in item['parsing_res_list']:\n",
        "        # Si la entidad detectada es una imagen, la almacenamos como array de números\n",
        "        if elem.label in ['image', 'chart']:\n",
        "            l.append(\n",
        "                DocumentElement(\n",
        "                    document_id=fn,\n",
        "                    page_number=page_number,\n",
        "                    layout_index=elem.index,\n",
        "                    label=elem.label,\n",
        "                    region_label=elem.order_label,\n",
        "                    bbox=elem.bbox,\n",
        "                    content=np.array(elem.image['img']).tolist()\n",
        "                )\n",
        "            )\n",
        "        elif elem.label in ['text', 'content', 'paragraph_title', 'table']:\n",
        "            l.append(\n",
        "                DocumentElement(\n",
        "                    document_id=fn,\n",
        "                    page_number=page_number,\n",
        "                    layout_index=elem.index,\n",
        "                    label=elem.label,\n",
        "                    region_label=elem.order_label,\n",
        "                    bbox=elem.bbox,\n",
        "                    content=elem.content\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "# Guardamos el objeto resultante\n",
        "with open(f'{fn}_ocr_result.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for line in l:\n",
        "        json_line = line.model_dump_json()\n",
        "        f.write(json_line + '\\n')"
      ],
      "metadata": {
        "id": "t-bqOy-Tgphw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 4: Vectorización\n",
        "\n",
        "El siguiente paso es generar una estrategia para _embeber_ nuestra información (bien sea de tipo textual o imágenes) en una serie de números, con la finalidad de poder establecer criterios de comparación mediante ciertas métricas, como la distancia coseno.\n",
        "\n",
        "![](https://www.mlflow.org/docs/3.0.1/assets/images/sentence-transformers-architecture-b83485a83e698e3e1576f44024570e81.png)\n"
      ],
      "metadata": {
        "id": "9TNVk-ohsIDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contexto: Sentence Transformers\n"
      ],
      "metadata": {
        "id": "roye3gRf9G3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una de las familias de modelos más populares para dicha tarea se conocen como **sentence-transformers**, y será [una variante de ellos](https://huggingface.co/jinaai/jina-clip-v2) los que empleemos en nuestro propósito.\n",
        "\n",
        "A fin de tener una mínima comprensión del funcionamiento de las variantes más clásicas de estos modelos, y sin intención de entrar en excesivos detalles técnicos, procedemos a ilustrar algunos detalles de su funcionamiento. Tradicionalmente, los primeros modelos de procesamiento de lenguaje natural que representaban lenguaje, estaban basados en frecuencias de determinadas palabras de un _vocabulario_ específico, basado en un _corpus_ de entrenamiento que se analiza con criterios estadísticos.\n",
        "\n",
        "![](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F96a71c0c08ba669c5a5a3af564cbffee81af9c6d-1920x1080.png&w=1920&q=75)\n",
        "\n",
        "Si bien estos modelos son sencillos de entrenar y poseen facilidad de convergencia _local_, somos nosotros los que estamos imponiendo un esquema de vectorización fijo (dando lugar a los modelos conocidos como _sparse_), sin dar margen de optimización al modelo a que **elija de forma abstracta qué variables emplear**. Estos últimos modelos se conocen como _densos_. En ellos, la vectorización no consiste directamente en criterios palpables, si no que en su lugar el modelo decide qué criterios (como caja negra) emplear para asignar esos números. Este entrenamiento es mucho más complejo y requiere de muchísimos más datos, pero da lugar a modelos más poderosos.\n",
        "\n",
        "En cierto sentido, podemos considerar entonces que disponemos de una _caja negra_ que _transforma_ nuestros textos en vectores, que son comparables mediante criterios como la similitud del coseno. Este principio de comparación se puede extender a otras modalidades de datos como imágenes o audios, dando lugar a modelos **multimodales**.\n",
        "\n",
        "![](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fd0e73377d123ccf0e910f4b971f6cb06bb87f200-1920x1080.png&w=1920&q=75)\n",
        "\n",
        "Esto nos va a permitir, en última instancia, obtener la similitud entre la pregunta de un usuario en un chat con cualquier tipo de información que hemos obtenido de nuestros documentos, como tablas o imágenes, no sólo texto.\n",
        "\n",
        "Finalmente, existe un último punto, que consiste precisamente en la elección de la longitud de esos vectores. Si bien damos _libertad_ al modelo durante el entrenamiento para decidir cómo configurar las variables numéricas, le imponemos que sea una cantidad fija. Este es un limitante bastante grande de cara a, por ejemplo, optimizar el equilibrio entre rendimiento y velocidad de las soluciones, para lo que surgió una estrategia adaptativa llamada **Matryoshka Representation Learning**, que permitía mantener una gran cantidad de precisión en ciertas tareas, acortando la longitud de los vectores, pudiendo así priorizar según nuestro caso qué configuración elegir.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:782/1*MAwYcyyo2mFC02bGA5-Xzw.png)\n",
        "\n",
        "Dado que la comparación de vectores más cortos es más rápida, para soluciones que necesiten una velocidad extrema, priorizaremos acortar la longitud de los mismos, y cuando la precisión sea vital, haremos lo contrario. Todos estos avances los vamos a poner en funcionamiento en nuestra implementación.\n"
      ],
      "metadata": {
        "id": "qCW_4AH8FqvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estrategia de _chunking_"
      ],
      "metadata": {
        "id": "aN4BzjWp-i5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos ahora cómo segmentar nuestra información basada en el planteamiento inicial. PAra ello, trataremos de forma independiente las imágenes/gráficos/tablas, y por otro lado el puro \"texto\". Con respecto a los primeros, los indexaremos de forma independiente y almacenaremos como un `string` tipo `base64`, que es como los modelos de lenguaje típicamente consumen estos elementos, y para el texto haremos una estrategia de _semantic chunking_, que ya describimos en otra sesión.\n",
        "\n",
        "Inicialmente, cargamos las dependencias, definimos las variables (que podéis sentiros libres de modificar para ver el comportamiento del sistema), cargamos el resultado del OCR, y finalmente inicializamos el modelo que vectoriza nuestra data:"
      ],
      "metadata": {
        "id": "_LcbSbe9FsOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencias\n",
        "import json\n",
        "import io\n",
        "import base64\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
        "from PIL import Image\n",
        "from typing import List, Union, Dict, Any, Optional\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Definimos las variables\n",
        "truncate_dim = 256\n",
        "min_chunk_size = 128\n",
        "max_chunk_size = 512\n",
        "similarity_threshold = 0.3\n",
        "format = 'PNG'\n",
        "\n",
        "# Leemos el resultado del OCR\n",
        "l = []\n",
        "with open(f\"{fn}_ocr_result.jsonl\", 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "      json_line = json.loads(line)\n",
        "      l.append(DocumentElement(**json_line))\n",
        "\n",
        "# Definimos una estructura que contenga nuestros elementos atómicos\n",
        "class Chunk(BaseModel):\n",
        "    document_id: str          # Identificador del documento al que pertenece.\n",
        "    label: str                # Etiqueta específica del modelo (ej: 'text', 'title', 'table').\n",
        "    region_label: str         # Etiqueta de la región a la que pertenece (ej: 'header', 'body').\n",
        "    embedding: List[float]    # Para construir el modelo, usamos texto o imagen según convenga\n",
        "    content: str              # Usamos base64 en caso sea imagen\n",
        "\n",
        "# Cargamos el modelo\n",
        "model = SentenceTransformer(\n",
        "    'jinaai/jina-clip-v2', trust_remote_code=True, truncate_dim=truncate_dim\n",
        ")"
      ],
      "metadata": {
        "id": "iL8X_8ZkGO51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es ahora cuando definimos la estrategia para crear nuestros `chunks` de datos:"
      ],
      "metadata": {
        "id": "PUm91l66NbhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "text = ''\n",
        "for elem in l:\n",
        "    if elem.label in ['image', 'chart']:\n",
        "        # Codificamos la imagen\n",
        "        img = Image.fromarray(np.array(elem.content, dtype=np.uint8))\n",
        "        emb = model.encode(img, normalize_embeddings=True)\n",
        "        # Convertimos la imagen a base64 (la forma en la que los LLMs consumen imágenes)\n",
        "        buffered = io.BytesIO()\n",
        "        img.save(buffered, format=format)\n",
        "        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "        mime_type = f\"image/{format.lower()}\"\n",
        "        base64_image = f\"data:{mime_type};base64,{img_str}\"\n",
        "        # Guardamos un chunk independiente al flujo de texto\n",
        "        chunks.append(\n",
        "            Chunk(\n",
        "                document_id=elem.document_id,\n",
        "                label=elem.label,\n",
        "                region_label=elem.region_label,\n",
        "                embedding=emb.tolist(),\n",
        "                content=base64_image,\n",
        "            )\n",
        "        )\n",
        "    elif elem.label in ['text', 'paragraph_title', 'content']:\n",
        "        tokenized_text_len = len(model.tokenizer.encode(text))\n",
        "        # Si el texto tiene una longitud menor a la que contemplamos como posible\n",
        "        if tokenized_text_len<min_chunk_size:\n",
        "            text += ' ' + elem.content\n",
        "        # Si el texto se encuentra en una longitud candidata para ser un chunk\n",
        "        elif ((tokenized_text_len>=min_chunk_size) & (tokenized_text_len<=max_chunk_size)):\n",
        "            embs = model.encode([text, elem.content], normalize_embeddings=True)\n",
        "            # Si la similitud es demasiado cercana\n",
        "            if float(np.dot(embs[0], embs[1]))>=similarity_threshold:\n",
        "                text += ' ' + elem.content\n",
        "            # Si los textos no se parecen lo suficiente\n",
        "            else:\n",
        "                # Guardamos el texto del chunk hasta la fecha\n",
        "                chunks.append(\n",
        "                    Chunk(\n",
        "                        document_id=elem.document_id,\n",
        "                        label='text',\n",
        "                        region_label='text',\n",
        "                        embedding=embs[0].tolist(),\n",
        "                        content=text,\n",
        "                    )\n",
        "                )\n",
        "                # Empezamos nuevo chunk\n",
        "                text = elem.content\n",
        "        # Si el texto ya excede la longitud establecida\n",
        "        else:\n",
        "            # Guardamos el texto del chunk hasta la fecha\n",
        "            emb = model.encode(text, normalize_embeddings=True)\n",
        "            chunks.append(\n",
        "                Chunk(\n",
        "                    document_id=elem.document_id,\n",
        "                    label='text',\n",
        "                    region_label='text',\n",
        "                    embedding=emb.tolist(),\n",
        "                    content=text,\n",
        "                )\n",
        "            )\n",
        "            # Empezamos nuevo chunk\n",
        "            text = elem.content"
      ],
      "metadata": {
        "id": "7DM7UHFHpzaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 5: Indexación en base de datos\n",
        "\n",
        "Finalmente, emplearemos un tipo especial de base de datos, que se ha popularizado mucho con los recientea avances de IA generativa, que son las **bases de datos de vectores**. Éstas implementan algoritmos específicos para búsqueda eficiente de información que viene almacenada en el formato que hemos preparado."
      ],
      "metadata": {
        "id": "IdD7mNf-NqKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencias\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "\n",
        "# Definimos un cliente local en memoria\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "# Creamos una nueva colección para albergar nuestros documentos\n",
        "client.create_collection(\n",
        "    collection_name=f\"{fn}_collection\",\n",
        "    vectors_config=VectorParams(size=truncate_dim, distance=Distance.COSINE),\n",
        ")"
      ],
      "metadata": {
        "id": "bUmGlktPN3tu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}