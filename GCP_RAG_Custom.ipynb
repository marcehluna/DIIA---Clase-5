{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "aTmjxcqCAjc7",
        "UF6A2H3HAg3i",
        "VXMaIFfOUne5",
        "roye3gRf9G3b"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Custom RAG con GCP\n",
        "\n",
        "## Introducción y contexto\n",
        "A lo largo de las sesiones anteriores, hemos explorado en profundidad los sistemas de Generación Aumentada por Recuperación (RAG), una arquitectura fundamental en el campo del Procesamiento del Lenguaje Natural (PLN) que permite a los modelos de lenguaje de gran tamaño (LLMs) generar respuestas más precisas y contextualizadas. Hemos aprendido a implementar y a interactuar con sistemas RAG utilizando servicios mayoritariamente autogestionados dentro del ecosistema de Google Cloud.\n",
        "\n",
        "Hasta ahora, nuestro enfoque se ha centrado en utilizar componentes pre-construidos donde la fase de indexación —el proceso de ingestar y preparar los datos para que el sistema de recuperación pueda actuar sobre ellos— se realizaba de forma casi automática. Hemos tratado con servicios totalmente gestionados y administrados, lo que nos ha permitido concentrarnos en la interacción entre el recuperador y el generador.\n",
        "\n",
        "Sin embargo, el mundo real necesita de soluciones más detalladas y precisas. La vasta mayoría de la información valiosa para las empresas y organizaciones se encuentra \"atrapada\" en formatos complejos como los documentos PDF. Estos archivos no son solo contenedores de texto; son un lienzo que combina texto, imágenes, tablas, columnas y una estructura visual que es trivial para un humano, pero inmensamente compleja para una máquina.\n",
        "Este cuaderno marca el siguiente paso en nuestra formación como especialistas en Deep Learning: nos adentraremos en el desafío de construir nosotros mismos el pipeline de indexación."
      ],
      "metadata": {
        "id": "n8LuCbjiL6Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descripción del problema\n",
        "\n",
        "Nuestro objetivo es procesar un documento PDF desde cero, extraer su contenido de manera inteligente y prepararlo para ser vectorizado e insertado en una base de datos vectorial. Este proceso es la piedra angular de cualquier sistema RAG robusto y escalable.\n",
        "\n",
        "Para lograrlo, no nos basta con una simple extracción de texto. Necesitamos entender la estructura del documento. Aquí es donde entran en juego dos tecnologías críticas:\n",
        "\n",
        "* Análisis de Disposición de Página (Layout Analysis): Esta parte del procesado nos especifica dónde y de qué tipo es el contenido de la página. Identifica párrafos, títulos, encabezados, pies de página, tablas y columnas. Entender el layout es crucial porque el orden y la agrupación del texto definen su contexto semántico. Por ejemplo, un texto en una columna debe leerse de arriba abajo antes de pasar a la siguiente columna, y los datos dentro de una tabla tienen una relación entre sí que se perdería con una lectura lineal.\n",
        "\n",
        "* Reconocimiento Óptico de Caracteres (OCR): Es el proceso de convertir imágenes de texto (como las que se encuentran en un PDF escaneado) en texto real que una máquina puede leer y procesar. Es la tarea siguiente a realizar una vez el _layout analysis_ ha hecho su parte.\n",
        "\n",
        "  ![](https://miro.medium.com/v2/resize:fit:1400/0*9WSDnnY0aI05R_cq.png)\n",
        "\n",
        "Para esta tarea, utilizaremos el servicio `PPStructureV3` de `PaddlePaddle`. Este no es un servicio de OCR convencional; está diseñado específicamente para el análisis inteligente de documentos y ofrece capacidades de vanguardia:\n",
        "\n",
        "1. Modelos Pre-entrenados: Utiliza modelos de Deep Learning entrenados en miles de millones de documentos, capaces de entender una variedad de layouts complejos sin necesidad de entrenamiento adicional.\n",
        "\n",
        "2. Identificación de Bloques: El servicio no devuelve una masa de texto desordenada. En su lugar, segmenta el documento en una estructura jerárquica: Páginas -> Bloques -> Párrafos -> Palabras -> Símbolos. Cada uno de estos elementos viene con su contenido de texto y sus coordenadas (bounding box).\n",
        "\n",
        "3. Manejo de Entidades: Es capaz de identificar entidades específicas como tablas, listas y otros elementos estructurales, permitiendo una extracción de datos mucho más granular y precisa.\n",
        "\n",
        "Al utilizar `PaddlePaddle`, podremos descomponer nuestro PDF en fragmentos de texto lógicos y coherentes. Estos fragmentos (o chunks) serán las unidades que posteriormente convertiremos en vectores para alimentar nuestra base de datos vectorial, asegurando que cada vector represente una idea o un concepto semánticamente cohesivo.\n",
        "\n",
        "A continuación, procederemos con la implementación práctica, pero previo a ellos es importante asegurarse de que tenemos habilitada la GPU en Colab.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJsR6G-TPHbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 0: Instalación de librerías"
      ],
      "metadata": {
        "id": "aTmjxcqCAjc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash\n",
        "# Install parallel\n",
        "sudo apt update && sudo apt install ghostscript && sudo apt install parallel\n",
        "\n",
        "# Install uv\n",
        "pip install uv\n",
        "\n",
        "# Install PaddlePaddle GPU 3.0.0 (for CUDA 12.6)\n",
        "uv pip install --pre paddlepaddle-gpu -i https://www.paddlepaddle.org.cn/packages/nightly/cu126/\n",
        "\n",
        "# Install PaddleOCR version that includes PPStructureV3\n",
        "uv pip install paddleocr==3.1.0 sentence-transformers einops timm pillow qdrant-client"
      ],
      "metadata": {
        "id": "iSh9UFvbKpF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320fcea5-7b04-4d94-86e3-9c4797ff1bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,516 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,575 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,282 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,202 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,085 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,772 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,154 kB]\n",
            "Fetched 32.3 MB in 3s (9,357 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 libgs9 libgs9-common\n",
            "  libidn12 libijs-0.35 libjbig2dec0 poppler-data\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf fonts-texgyre\n",
            "  ghostscript-x poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho\n",
            "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript libgs9\n",
            "  libgs9-common libidn12 libijs-0.35 libjbig2dec0 poppler-data\n",
            "0 upgraded, 10 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 16.7 MB of archives.\n",
            "After this operation, 63.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.12 [753 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.12 [5,031 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ghostscript amd64 9.55.0~dfsg1-0ubuntu5.12 [49.4 kB]\n",
            "Fetched 16.7 MB in 1s (23.6 MB/s)\n",
            "Selecting previously unselected package fonts-droid-fallback.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126284 files and directories currently installed.)\r\n",
            "Preparing to unpack .../0-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\r\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\r\n",
            "Selecting previously unselected package poppler-data.\r\n",
            "Preparing to unpack .../1-poppler-data_0.4.11-1_all.deb ...\r\n",
            "Unpacking poppler-data (0.4.11-1) ...\r\n",
            "Selecting previously unselected package fonts-noto-mono.\r\n",
            "Preparing to unpack .../2-fonts-noto-mono_20201225-1build1_all.deb ...\r\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\r\n",
            "Selecting previously unselected package fonts-urw-base35.\r\n",
            "Preparing to unpack .../3-fonts-urw-base35_20200910-1_all.deb ...\r\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\r\n",
            "Selecting previously unselected package libgs9-common.\r\n",
            "Preparing to unpack .../4-libgs9-common_9.55.0~dfsg1-0ubuntu5.12_all.deb ...\r\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.12) ...\r\n",
            "Selecting previously unselected package libidn12:amd64.\r\n",
            "Preparing to unpack .../5-libidn12_1.38-4ubuntu1_amd64.deb ...\r\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\r\n",
            "Selecting previously unselected package libijs-0.35:amd64.\r\n",
            "Preparing to unpack .../6-libijs-0.35_0.35-15build2_amd64.deb ...\r\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\r\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\r\n",
            "Preparing to unpack .../7-libjbig2dec0_0.19-3build2_amd64.deb ...\r\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\r\n",
            "Selecting previously unselected package libgs9:amd64.\r\n",
            "Preparing to unpack .../8-libgs9_9.55.0~dfsg1-0ubuntu5.12_amd64.deb ...\r\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.12) ...\r\n",
            "Selecting previously unselected package ghostscript.\r\n",
            "Preparing to unpack .../9-ghostscript_9.55.0~dfsg1-0ubuntu5.12_amd64.deb ...\r\n",
            "Unpacking ghostscript (9.55.0~dfsg1-0ubuntu5.12) ...\r\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\r\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\r\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\r\n",
            "Setting up poppler-data (0.4.11-1) ...\r\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\r\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\r\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\r\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.12) ...\r\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.12) ...\r\n",
            "Setting up ghostscript (9.55.0~dfsg1-0ubuntu5.12) ...\r\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\r\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
            "\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  sysstat\n",
            "Suggested packages:\n",
            "  ash csh fish ksh tcsh zsh isag\n",
            "The following NEW packages will be installed:\n",
            "  parallel sysstat\n",
            "0 upgraded, 2 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 2,434 kB of archives.\n",
            "After this operation, 4,521 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 parallel all 20210822+ds-2 [1,947 kB]\n",
            "Fetched 2,434 kB in 0s (18.0 MB/s)\n",
            "Selecting previously unselected package sysstat.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 127387 files and directories currently installed.)\r\n",
            "Preparing to unpack .../sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\r\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\r\n",
            "Selecting previously unselected package parallel.\r\n",
            "Preparing to unpack .../parallel_20210822+ds-2_all.deb ...\r\n",
            "Adding 'diversion of /usr/bin/parallel to /usr/bin/parallel.moreutils by parallel'\r\n",
            "Adding 'diversion of /usr/share/man/man1/parallel.1.gz to /usr/share/man/man1/parallel.moreutils.1.gz by parallel'\r\n",
            "Unpacking parallel (20210822+ds-2) ...\r\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\r\n",
            "debconf: unable to initialize frontend: Dialog\r\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\r\n",
            "debconf: falling back to frontend: Readline\r\n",
            "\r\n",
            "Creating config file /etc/default/sysstat with new version\r\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\r\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer → /lib/systemd/system/sysstat-collect.timer.\r\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer → /lib/systemd/system/sysstat-summary.timer.\r\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\r\n",
            "Setting up parallel (20210822+ds-2) ...\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            "Collecting uv\n",
            "  Downloading uv-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.8/18.8 MB 27.9 MB/s eta 0:00:00\n",
            "Installing collected packages: uv\n",
            "Successfully installed uv-0.8.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Using Python 3.11.13 environment at: /usr\n",
            "Resolved 29 packages in 34.47s\n",
            "Downloading nvidia-curand-cu12 (53.7MiB)\n",
            "Downloading nvidia-cuda-cupti-cu12 (8.5MiB)\n",
            "Downloading nvidia-cufile-cu12 (1.1MiB)\n",
            "Downloading nvidia-cusparselt-cu12 (149.5MiB)\n",
            "Downloading nvidia-nccl-cu12 (192.0MiB)\n",
            "Downloading nvidia-cuda-nvrtc-cu12 (22.6MiB)\n",
            "Downloading nvidia-cuda-cccl-cu12 (2.7MiB)\n",
            "Downloading nvidia-cudnn-cu12 (544.5MiB)\n",
            "Downloading nvidia-cusolver-cu12 (150.9MiB)\n",
            "Downloading nvidia-nvjitlink-cu12 (18.8MiB)\n",
            "Downloading paddlepaddle-gpu (1.6GiB)\n",
            "Downloading nvidia-cusparse-cu12 (206.5MiB)\n",
            "Downloading nvidia-cublas-cu12 (374.9MiB)\n",
            "Downloading nvidia-cufft-cu12 (190.9MiB)\n",
            " Downloading nvidia-curand-cu12\n",
            " Downloading nvidia-cufile-cu12\n",
            " Downloading nvidia-cuda-cccl-cu12\n",
            " Downloading nvidia-cuda-cupti-cu12\n",
            " Downloading nvidia-cuda-nvrtc-cu12\n",
            " Downloading nvidia-nvjitlink-cu12\n",
            " Downloading nvidia-cusolver-cu12\n",
            " Downloading nvidia-cusparselt-cu12\n",
            " Downloading nvidia-cufft-cu12\n",
            " Downloading nvidia-nccl-cu12\n",
            " Downloading nvidia-cusparse-cu12\n",
            " Downloading nvidia-cublas-cu12\n",
            " Downloading nvidia-cudnn-cu12\n",
            " Downloading paddlepaddle-gpu\n",
            "Prepared 17 packages in 3m 25s\n",
            "Uninstalled 14 packages in 38ms\n",
            "Installed 17 packages in 142ms\n",
            " - nvidia-cublas-cu12==12.5.3.2\n",
            " + nvidia-cublas-cu12==12.6.4.1\n",
            " + nvidia-cuda-cccl-cu12==12.6.77\n",
            " - nvidia-cuda-cupti-cu12==12.5.82\n",
            " + nvidia-cuda-cupti-cu12==12.6.80\n",
            " - nvidia-cuda-nvrtc-cu12==12.5.82\n",
            " + nvidia-cuda-nvrtc-cu12==12.6.77\n",
            " - nvidia-cuda-runtime-cu12==12.5.82\n",
            " + nvidia-cuda-runtime-cu12==12.6.77\n",
            " - nvidia-cudnn-cu12==9.3.0.75\n",
            " + nvidia-cudnn-cu12==9.5.1.17\n",
            " - nvidia-cufft-cu12==11.2.3.61\n",
            " + nvidia-cufft-cu12==11.3.0.4\n",
            " + nvidia-cufile-cu12==1.11.1.6\n",
            " - nvidia-curand-cu12==10.3.6.82\n",
            " + nvidia-curand-cu12==10.3.7.77\n",
            " - nvidia-cusolver-cu12==11.6.3.83\n",
            " + nvidia-cusolver-cu12==11.7.1.2\n",
            " - nvidia-cusparse-cu12==12.5.1.3\n",
            " + nvidia-cusparse-cu12==12.5.4.2\n",
            " - nvidia-cusparselt-cu12==0.6.2\n",
            " + nvidia-cusparselt-cu12==0.6.3\n",
            " - nvidia-nccl-cu12==2.21.5\n",
            " + nvidia-nccl-cu12==2.25.1\n",
            " - nvidia-nvjitlink-cu12==12.5.82\n",
            " + nvidia-nvjitlink-cu12==12.6.85\n",
            " - nvidia-nvtx-cu12==12.4.127\n",
            " + nvidia-nvtx-cu12==12.6.77\n",
            " - opt-einsum==3.4.0\n",
            " + opt-einsum==3.3.0\n",
            " + paddlepaddle-gpu==3.0.0.dev20250729\n",
            "Using Python 3.11.13 environment at: /usr\n",
            "Resolved 121 packages in 1.30s\n",
            "   Building gputil==1.4.0\n",
            "Downloading nvidia-nccl-cu12 (179.9MiB)\n",
            "Downloading nvidia-cublas-cu12 (346.6MiB)\n",
            "Downloading nvidia-cufft-cu12 (201.7MiB)\n",
            "Downloading nvidia-cusparselt-cu12 (143.1MiB)\n",
            "Downloading paddlex (1.6MiB)\n",
            "Downloading pypdfium2 (2.7MiB)\n",
            "Downloading opencv-contrib-python (65.5MiB)\n",
            "Downloading nvidia-cuda-cupti-cu12 (13.2MiB)\n",
            "Downloading nvidia-cusolver-cu12 (122.0MiB)\n",
            "Downloading nvidia-nvjitlink-cu12 (20.1MiB)\n",
            "Downloading langchain-community (2.4MiB)\n",
            "Downloading nvidia-curand-cu12 (53.7MiB)\n",
            "Downloading nvidia-cuda-nvrtc-cu12 (23.5MiB)\n",
            "Downloading nvidia-cusparse-cu12 (197.8MiB)\n",
            "Downloading nvidia-cudnn-cu12 (634.0MiB)\n",
            "      Built gputil==1.4.0\n",
            " Downloading pypdfium2\n",
            " Downloading nvidia-cuda-cupti-cu12\n",
            " Downloading paddlex\n",
            " Downloading nvidia-nvjitlink-cu12\n",
            " Downloading nvidia-cuda-nvrtc-cu12\n",
            " Downloading langchain-community\n",
            " Downloading nvidia-curand-cu12\n",
            " Downloading opencv-contrib-python\n",
            " Downloading nvidia-cusolver-cu12\n",
            " Downloading nvidia-cusparselt-cu12\n",
            " Downloading nvidia-nccl-cu12\n",
            " Downloading nvidia-cusparse-cu12\n",
            " Downloading nvidia-cufft-cu12\n",
            " Downloading nvidia-cublas-cu12\n",
            " Downloading nvidia-cudnn-cu12\n",
            "Prepared 36 packages in 30.78s\n",
            "Uninstalled 14 packages in 84ms\n",
            "Installed 36 packages in 126ms\n",
            " + colorlog==6.9.0\n",
            " + cssselect==1.3.0\n",
            " + cssutils==2.11.1\n",
            " + dataclasses-json==0.6.7\n",
            " + ftfy==6.3.1\n",
            " + gputil==1.4.0\n",
            " + httpx-sse==0.4.1\n",
            " + langchain-community==0.3.27\n",
            " + langchain-openai==0.3.28\n",
            " + marshmallow==3.26.1\n",
            " + mypy-extensions==1.1.0\n",
            " - nvidia-cublas-cu12==12.6.4.1\n",
            " + nvidia-cublas-cu12==12.4.5.8\n",
            " - nvidia-cuda-cupti-cu12==12.6.80\n",
            " + nvidia-cuda-cupti-cu12==12.4.127\n",
            " - nvidia-cuda-nvrtc-cu12==12.6.77\n",
            " + nvidia-cuda-nvrtc-cu12==12.4.127\n",
            " - nvidia-cuda-runtime-cu12==12.6.77\n",
            " + nvidia-cuda-runtime-cu12==12.4.127\n",
            " - nvidia-cudnn-cu12==9.5.1.17\n",
            " + nvidia-cudnn-cu12==9.1.0.70\n",
            " - nvidia-cufft-cu12==11.3.0.4\n",
            " + nvidia-cufft-cu12==11.2.1.3\n",
            " - nvidia-curand-cu12==10.3.7.77\n",
            " + nvidia-curand-cu12==10.3.5.147\n",
            " - nvidia-cusolver-cu12==11.7.1.2\n",
            " + nvidia-cusolver-cu12==11.6.1.9\n",
            " - nvidia-cusparse-cu12==12.5.4.2\n",
            " + nvidia-cusparse-cu12==12.3.1.170\n",
            " - nvidia-cusparselt-cu12==0.6.3\n",
            " + nvidia-cusparselt-cu12==0.6.2\n",
            " - nvidia-nccl-cu12==2.25.1\n",
            " + nvidia-nccl-cu12==2.21.5\n",
            " - nvidia-nvjitlink-cu12==12.6.85\n",
            " + nvidia-nvjitlink-cu12==12.4.127\n",
            " - nvidia-nvtx-cu12==12.6.77\n",
            " + nvidia-nvtx-cu12==12.4.127\n",
            " - opencv-contrib-python==4.12.0.88\n",
            " + opencv-contrib-python==4.10.0.84\n",
            " + paddleocr==3.1.0\n",
            " + paddlex==3.1.3\n",
            " + premailer==3.10.0\n",
            " + pyclipper==1.3.0.post6\n",
            " + pydantic-settings==2.10.1\n",
            " + pypdfium2==4.30.0\n",
            " + python-dotenv==1.1.1\n",
            " + ruamel-yaml==0.18.14\n",
            " + ruamel-yaml-clib==0.2.12\n",
            " + typing-inspect==0.9.0\n",
            " + ujson==5.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: Lectura del PDF\n",
        "\n",
        "En primer lugar, vamos a definir un método que nos permita, a partir de una determinada ruta en la que especifiquemos un archivo `.pdf`, dividirlo en páginas independientes, y convertir cada una de ellas a `.jpg`."
      ],
      "metadata": {
        "id": "UF6A2H3HAg3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import logging as log\n",
        "from typing import List, Dict\n",
        "\n",
        "log.basicConfig(level=log.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def convert_pdf_to_images(pdf_path: str, output_dir: str = \"images\", dpi: int = 300) -> Dict[str, List[str]]:\n",
        "\n",
        "    filepaths = []\n",
        "    if os.path.isfile(pdf_path) and pdf_path.lower().endswith(\".pdf\"):\n",
        "        filepaths.append(pdf_path)\n",
        "    elif os.path.isdir(pdf_path):\n",
        "        for root, _, filenames in os.walk(pdf_path):\n",
        "            for filename in filenames:\n",
        "                if filename.lower().endswith(\".pdf\"):\n",
        "                    filepaths.append(os.path.join(root, filename))\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No valid PDF files found at the specified path: {pdf_path}\")\n",
        "\n",
        "    if not filepaths:\n",
        "        raise FileNotFoundError(f\"No PDF files were found to process in {pdf_path}.\")\n",
        "\n",
        "    file2image = {}\n",
        "    for filepath in filepaths:\n",
        "        name = os.path.splitext(os.path.basename(filepath))[0]\n",
        "        output_path = os.path.join(output_dir, name)\n",
        "\n",
        "        if os.path.isdir(output_path) and os.listdir(output_path):\n",
        "            log.info(f\"Images for '{name}' already exist. Skipping conversion.\")\n",
        "            file2image[name] = sorted([os.path.join(output_path, img) for img in os.listdir(output_path) if img.endswith(\".jpg\")])\n",
        "            continue\n",
        "\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        log.info(f\"Converting '{name}' to images at {dpi} DPI...\")\n",
        "        cmd = [\n",
        "            \"gs\", \"-dNOPAUSE\", \"-dBATCH\", \"-sDEVICE=jpeg\",\n",
        "            f\"-r{dpi}\", f\"-sOutputFile={os.path.join(output_path, 'page_%03d.jpg')}\",\n",
        "            filepath,\n",
        "        ]\n",
        "        try:\n",
        "            subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            log.error(f\"Ghostscript failed for {filepath}: {e.stderr}\")\n",
        "            raise RuntimeError(f\"PDF to image conversion failed for {filepath}: {e}\")\n",
        "\n",
        "        file2image[name] = sorted([os.path.join(output_path, img) for img in os.listdir(output_path) if img.endswith(\".jpg\")])\n",
        "    return file2image"
      ],
      "metadata": {
        "id": "gRGjFbR0Bnil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos un `.pdf` como referencia:"
      ],
      "metadata": {
        "id": "c_W-rC4tjee0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://arxiv.org/pdf/2505.09388'\n",
        "fn = os.path.splitext(os.path.basename(url))[0]\n",
        "\n",
        "# Use stream=True to not load the whole content into memory\n",
        "with requests.get(url, stream=True, timeout=10) as r:\n",
        "    r.raise_for_status()\n",
        "\n",
        "    # Open the file in binary write mode\n",
        "    with open(f\"{fn}.pdf\", 'wb') as f:\n",
        "        # Download the file in chunks\n",
        "        for chunk in r.iter_content(chunk_size=8192): # 8KB chunks\n",
        "            f.write(chunk)\n",
        "\n",
        "print(f\"Successfully downloaded '{fn}' via streaming.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbUnFOQhjh5q",
        "outputId": "1f82b833-daa3-4a4b-9c0e-b6d35c7a5a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded '2505' via streaming.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez definido el método, nos aseguramos de que generamos un directorio donde las imágenes puedan ser almacenadas, y lo empleamos:"
      ],
      "metadata": {
        "id": "QUEBwLi_GQhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Especificamos la ruta donde se aloja el PDF\n",
        "pdf_path = f\"{fn}.pdf\"\n",
        "# Nos aseguramos de que generamos un directorio para las imágenes\n",
        "output_dir = 'images'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Ejecutamos el método construído\n",
        "doc2images = convert_pdf_to_images(pdf_path, output_dir)"
      ],
      "metadata": {
        "id": "ynbUfJ4NGQK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Procesamiento `OCR`\n",
        "\n",
        "Como ya indicamos, `PaddlePaddle` dispone de sistemas de alto rendimiento para nuestro propósito, incluyendo complejos _pipelines_ que distinguen entre tipos de objetos, posicionamiento, idiomas, entre otros:\n",
        "\n",
        "![](https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/PP-StructureV3/algorithm_ppstructurev3.png)\n",
        "\n",
        "Dado que, además, es un servicio de código abierto, podemos emplearlo cómodamente en nuestras soluciones. A continuación, mostramos cómo hacerlo:"
      ],
      "metadata": {
        "id": "VXMaIFfOUne5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from paddleocr import PPStructureV3\n",
        "\n",
        "pipeline = PPStructureV3(\n",
        "    layout_detection_model_name=\"PP-DocLayout_plus-L\",\n",
        "    text_recognition_model_name=\"PP-OCRv5_server_rec\",\n",
        "    use_doc_orientation_classify=True,\n",
        "    use_doc_unwarping=True,\n",
        "    use_textline_orientation=True,\n",
        "    device=\"gpu\",\n",
        ")\n",
        "\n",
        "output = pipeline.predict(doc2images[fn])"
      ],
      "metadata": {
        "id": "VILUDbceocXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Análisis del resultado e indexación\n",
        "\n",
        "El resultado de este análisis es tan detallado como el proceso en sí. En concreto, la salida del `pipeline` será una lista de diccionarios, en la que nos interesa particularmente el elemento `parsing_res_list`. En él, tendremos toda la información disponible; en particular, dicha información estará tipificada en los siguientes conjuntos:\n",
        "\n",
        "* header\n",
        "* doc_title\n",
        "* text\n",
        "* paragraph_title\n",
        "* figure_title\n",
        "* image\n",
        "* footer\n",
        "* content\n",
        "* number\n",
        "* table\n",
        "* footnote\n",
        "* chart\n",
        "\n"
      ],
      "metadata": {
        "id": "IMJSAUWjWjkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A fin de encontrar un buen equilibrio entre eficacia y eficiencia, analizaremos los contenidos más esenciales, que son `text`, `content`, `paragraph_title`, `image`, `table` y `chart`. Teniendo en consideración que nuestro objetivo final es vectorizar nuestro documento (previo paso de indexación o _chunking_), la estrategia que tomamos es:\n",
        "\n",
        "1. Para los elementos de texto (que incluye a `text`, `content` y `paragraph_title`), simplemente tomamos el contenido y lo vectorizamos. Un ejemplo de lo que el proceso `OCR` nos devuelve sobre estos elementos es:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    'label': 'text',\n",
        "    'order_label': 'normal_text',\n",
        "    'bbox': [166, 952, 1633, 1192],\n",
        "    'content': 'An Architecture for Building Agentic Applications in the Enterprise ',\n",
        "    'width': np.float32(1466.4224),\n",
        "    'height': np.float32(239.9646),\n",
        "    'area': 351889.4552630186,\n",
        "    'num_of_lines': 2,\n",
        "    'image': None,\n",
        "    'index': 0,\n",
        "    'order_index': 2,\n",
        "    'text_line_width': np.float64(1328.0),\n",
        "    'text_line_height': np.float64(121.5),\n",
        "    'child_blocks': [],\n",
        "    'direction': 'horizontal',\n",
        "    'secondary_direction': 'vertical',\n",
        "    #...\n",
        "  }\n",
        "  ```\n",
        "2. Para elementos de tipo tabla, tendríamos la opción de vectorizar el texto resultado de nuestro sistema OCR, o bien vectorizar directamente la tabla como una imagen. En este caso, tomaremos la priemra aproximación al problema por cuestiones de eficiencia, aunque para tablas muy complejas podremos siempre optar por la segunda. Nuestro proceso nos retorna de estos objetos lo siguiente:\n",
        "\n",
        "  ```json\n",
        "    {\n",
        "      'label': 'table',\n",
        "      'order_label': 'vision',\n",
        "      'bbox': [297, 497, 1498, 1953],\n",
        "      'content': '<html><body><table><tbody><tr><td>Developer or Provider</     td><td>Model or Product</td><td>Release Date</td><td>Description</td></     tr><tr><td>OpenAl</td><td>GPT-3</td><td>May 2020</  td><td>175billionparameter   LLMwith2048token contextwindow</td></  tr><tr><td>OpenAl</td><td>ChatGPT</   td><td>November 2022</td><td>Consumer   chatbotapplication,poweredby GPT-3.   5Turbo</td></tr><tr><td>Microsoft   Azure</td><td>OpenAl Service</ td><td>January  2023</td><td>Managed   serviceofferingLLMs fromOpenAI</td></ tr><tr><td>Amazon Web  Services</ td><td>Bedrock</td><td>September 2023</ td><td>Managed serviceoffering   LLMs from various developers</td></  tr><tr><td>Dataiku</td><td>LLMMesh</    td><td>September 2023</ td><td>CommercialLLMMeshoffering forconnecting to   LLMs   and buildinq agentic  applications in the enterprise</td></    tr><tr><td>Databricks</td><td>DBRX</  td><td>March 2024</ td><td>Open-weiqhts  mixture ofexperts model with 132B   total parameters  and 32k-token input context   window,licensed forcommercial  use</td></  tr><tr><td>Meta</td><td>LLaMA3 (8B,70B) </td><td>April2024</  td><td>UpdatedLLMwith4096-tokeninputcontext window,  withupdated     licenseallowingcertain commercial uses</td></tr><tr><td>Mistral</     td><td>Mixtral 8x22B</td><td>April 2024</td><td>Open-weights mixture    ofexperts   model with up to141Bparameters and64k-inputcontext window,  licensed  forcommercial use</td></tr><tr><td>OpenAl</td><td>GPT-40</   td><td>May 2024</  td><td>Multimodal LLM supporting voice-to-voice  generation  and128k-token input  context window</td></tr><tr><td>OpenAl</  td><td>01</ td><td>September 2024</ td><td>Reasoningmodel withbuilt-in  chain-of-thought  for complex scientificand  mathematical problems</td></  tr><tr><td>DeepSeek</ td><td>R1</td><td>January   2025</td><td>Open-source  reasoning model (MIT  license) optimized for math,  coding, and logic</td></ tr></tbody></table></  body></html>',\n",
        "      'width': np.float32(1200.5344),\n",
        "      'height': np.float32(1456.0574),\n",
        "      'area': 1748046.9994115233,\n",
        "      'num_of_lines': 1,\n",
        "      'image': {'path': 'imgs/img_in_table_box_297_497_1498_1953.jpg',\n",
        "       'img': <PIL.Image.Image image mode=RGB size=1201x1456>},\n",
        "      'index': 0,\n",
        "      'child_blocks': [],\n",
        "      'direction': 'horizontal',\n",
        "      'secondary_direction': 'vertical',\n",
        "      #...\n",
        "    }\n",
        "  ```\n",
        "\n",
        "3. Para elementos tipo imagen (entre los que están `image` y `chart`), directamente accederemos al atributo `image` para tomar la imagen como _array_ numérico:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "  'label': 'image',\n",
        "   'order_label': 'vision',\n",
        "   'bbox': [121, 1709, 1676, 2439],\n",
        "   'content': 'The Universal Al Platform T \\nControl Agents Enterprise  Orchestration Continuous Optimization Central Govermance Multi-Agents \\nAgent  Observabillity Strategic Oversight Multi-Models \\nAnalytics Models   \\nSecurity & Guardrails Agent Performance Risk Monitoring \\nCreate Agents   Knowledge Worker Developer Data & Al Infrastructure Third-Party Agents & Tools  A aws W ',\n",
        "   'seg_start_coordinate': np.float64(694.0),\n",
        "   'seg_end_coordinate': np.float64(1548.0),\n",
        "   'width': np.float32(1554.9723),\n",
        "   'height': np.float32(729.7499),\n",
        "   'area': 1134740.8388400525,\n",
        "   'num_of_lines': 1,\n",
        "   'image': {'path': 'imgs/img_in_image_box_121_1709_1676_2439.jpg',\n",
        "    'img': <PIL.Image.Image image mode=RGB size=1555x730>},\n",
        "   'index': 3,\n",
        "   'order_index': None,\n",
        "   'text_line_width': np.float64(614.4166666666666),\n",
        "   'text_line_height': np.float64(34.25),\n",
        "   'child_blocks': [],\n",
        "   'direction': 'horizontal',\n",
        "   'secondary_direction': 'vertical',\n",
        "   #...\n",
        "   }\n",
        "  ```\n",
        "\n",
        "  Observamos en particular que el proceso `OCR` hace un esfuerzo por proporcionar información textual sobre imágenes, gráficas y tablas.\n",
        "\n",
        "Vamos a proceder ahora a almacenar la información que estrictamente necesitamos del proceso `OCR`. Concretamente, para hacer una indexación completa, necesitaremos la información que mostramos en la siguiente estructura de datos:"
      ],
      "metadata": {
        "id": "GD6V1JmeFiLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencias\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
        "from PIL import Image\n",
        "from typing import List, Union, Dict, Any, Optional\n",
        "import numpy as np\n",
        "\n",
        "# Definimos una estructura que contenga nuestros elementos atómicos\n",
        "class DocumentElement(BaseModel):\n",
        "    \"\"\"\n",
        "    Representa un único elemento atómico detectado en un documento, como un\n",
        "    bloque de texto, un título, una tabla o una imagen.\n",
        "\n",
        "    Esta clase utiliza Pydantic para validar los datos y asegurar que cada\n",
        "    elemento tenga la estructura correcta.\n",
        "    \"\"\"\n",
        "\n",
        "    document_id: str          # Identificador del documento al que pertenece.\n",
        "    page_number: int          # Número de la página donde se encuentra el elemento.\n",
        "    layout_index: int         # Índice del elemento dentro del análisis del layout.\n",
        "    label: str                # Etiqueta específica del modelo (ej: 'text', 'title', 'table').\n",
        "    region_label: str         # Etiqueta de la región a la que pertenece (ej: 'header', 'body').\n",
        "\n",
        "    # El cuadro delimitador (bounding box) [x1, y1, x2, y2].\n",
        "    # Field(...) indica que es obligatorio. min_items y max_items aseguran que siempre tenga 4 coordenadas.\n",
        "    bbox: List[int] = Field(..., min_items=4, max_items=4)\n",
        "\n",
        "    # El contenido del elemento. Puede ser texto (str) o una imagen (np.array).\n",
        "    content: Union[str, List[float], List[List[float]], List[List[List[float]]]]"
      ],
      "metadata": {
        "id": "mUH_izE6ThsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almacenamos ahora en una lista los resultados:"
      ],
      "metadata": {
        "id": "xLft8w05oM_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una lista vacía donde iremos acumulando los `chunks`\n",
        "l = []\n",
        "\n",
        "# Iteramos sobre cada página\n",
        "for page_number, item in enumerate(output):\n",
        "    # Iteramos sobre cada elemento de salida\n",
        "    for elem in item['parsing_res_list']:\n",
        "        # Si la entidad detectada es una imagen, la almacenamos como array de números\n",
        "        if elem.label in ['image', 'chart']:\n",
        "            l.append(\n",
        "                DocumentElement(\n",
        "                    document_id=fn,\n",
        "                    page_number=page_number,\n",
        "                    layout_index=elem.index,\n",
        "                    label=elem.label,\n",
        "                    region_label=elem.order_label,\n",
        "                    bbox=elem.bbox,\n",
        "                    content=np.array(elem.image['img']).tolist()\n",
        "                )\n",
        "            )\n",
        "        elif elem.label in ['text', 'content', 'paragraph_title', 'table']:\n",
        "            l.append(\n",
        "                DocumentElement(\n",
        "                    document_id=fn,\n",
        "                    page_number=page_number,\n",
        "                    layout_index=elem.index,\n",
        "                    label=elem.label,\n",
        "                    region_label=elem.order_label,\n",
        "                    bbox=elem.bbox,\n",
        "                    content=elem.content\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "# Guardamos el objeto resultante\n",
        "with open(f'{fn}_ocr_result.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for line in l:\n",
        "        json_line = line.model_dump_json()\n",
        "        f.write(json_line + '\\n')"
      ],
      "metadata": {
        "id": "t-bqOy-Tgphw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 4: Vectorización\n",
        "\n",
        "El siguiente paso es generar una estrategia para _embeber_ nuestra información (bien sea de tipo textual o imágenes) en una serie de números, con la finalidad de poder establecer criterios de comparación mediante ciertas métricas, como la distancia coseno.\n",
        "\n",
        "![](https://www.mlflow.org/docs/3.0.1/assets/images/sentence-transformers-architecture-b83485a83e698e3e1576f44024570e81.png)\n"
      ],
      "metadata": {
        "id": "9TNVk-ohsIDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contexto: Sentence Transformers\n"
      ],
      "metadata": {
        "id": "roye3gRf9G3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una de las familias de modelos más populares para dicha tarea se conocen como **sentence-transformers**, y será [una variante de ellos](https://huggingface.co/jinaai/jina-clip-v2) los que empleemos en nuestro propósito.\n",
        "\n",
        "A fin de tener una mínima comprensión del funcionamiento de las variantes más clásicas de estos modelos, y sin intención de entrar en excesivos detalles técnicos, procedemos a ilustrar algunos detalles de su funcionamiento. Tradicionalmente, los primeros modelos de procesamiento de lenguaje natural que representaban lenguaje, estaban basados en frecuencias de determinadas palabras de un _vocabulario_ específico, basado en un _corpus_ de entrenamiento que se analiza con criterios estadísticos.\n",
        "\n",
        "![](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F96a71c0c08ba669c5a5a3af564cbffee81af9c6d-1920x1080.png&w=1920&q=75)\n",
        "\n",
        "Si bien estos modelos son sencillos de entrenar y poseen facilidad de convergencia _local_, somos nosotros los que estamos imponiendo un esquema de vectorización fijo (dando lugar a los modelos conocidos como _sparse_), sin dar margen de optimización al modelo a que **elija de forma abstracta qué variables emplear**. Estos últimos modelos se conocen como _densos_. En ellos, la vectorización no consiste directamente en criterios palpables, si no que en su lugar el modelo decide qué criterios (como caja negra) emplear para asignar esos números. Este entrenamiento es mucho más complejo y requiere de muchísimos más datos, pero da lugar a modelos más poderosos.\n",
        "\n",
        "En cierto sentido, podemos considerar entonces que disponemos de una _caja negra_ que _transforma_ nuestros textos en vectores, que son comparables mediante criterios como la similitud del coseno. Este principio de comparación se puede extender a otras modalidades de datos como imágenes o audios, dando lugar a modelos **multimodales**.\n",
        "\n",
        "![](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fd0e73377d123ccf0e910f4b971f6cb06bb87f200-1920x1080.png&w=1920&q=75)\n",
        "\n",
        "Esto nos va a permitir, en última instancia, obtener la similitud entre la pregunta de un usuario en un chat con cualquier tipo de información que hemos obtenido de nuestros documentos, como tablas o imágenes, no sólo texto.\n",
        "\n",
        "Finalmente, existe un último punto, que consiste precisamente en la elección de la longitud de esos vectores. Si bien damos _libertad_ al modelo durante el entrenamiento para decidir cómo configurar las variables numéricas, le imponemos que sea una cantidad fija. Este es un limitante bastante grande de cara a, por ejemplo, optimizar el equilibrio entre rendimiento y velocidad de las soluciones, para lo que surgió una estrategia adaptativa llamada **Matryoshka Representation Learning**, que permitía mantener una gran cantidad de precisión en ciertas tareas, acortando la longitud de los vectores, pudiendo así priorizar según nuestro caso qué configuración elegir.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:782/1*MAwYcyyo2mFC02bGA5-Xzw.png)\n",
        "\n",
        "Dado que la comparación de vectores más cortos es más rápida, para soluciones que necesiten una velocidad extrema, priorizaremos acortar la longitud de los mismos, y cuando la precisión sea vital, haremos lo contrario. Todos estos avances los vamos a poner en funcionamiento en nuestra implementación.\n"
      ],
      "metadata": {
        "id": "qCW_4AH8FqvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estrategia de _chunking_"
      ],
      "metadata": {
        "id": "aN4BzjWp-i5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos ahora cómo segmentar nuestra información basada en el planteamiento inicial. PAra ello, trataremos de forma independiente las imágenes/gráficos/tablas, y por otro lado el puro \"texto\". Con respecto a los primeros, los indexaremos de forma independiente y almacenaremos como un `string` tipo `base64`, que es como los modelos de lenguaje típicamente consumen estos elementos, y para el texto haremos una estrategia de _semantic chunking_, que ya describimos en otra sesión.\n",
        "\n",
        "Inicialmente, cargamos las dependencias, definimos las variables (que podéis sentiros libres de modificar para ver el comportamiento del sistema), cargamos el resultado del OCR, y finalmente inicializamos el modelo que vectoriza nuestra data:"
      ],
      "metadata": {
        "id": "_LcbSbe9FsOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencias\n",
        "import json\n",
        "import io\n",
        "import base64\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict\n",
        "from PIL import Image\n",
        "from typing import List, Union, Dict, Any, Optional\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Definimos las variables\n",
        "truncate_dim = 256\n",
        "min_chunk_size = 128\n",
        "max_chunk_size = 512\n",
        "similarity_threshold = 0.3\n",
        "format = 'PNG'\n",
        "\n",
        "# Leemos el resultado del OCR\n",
        "l = []\n",
        "with open(f\"{fn}_ocr_result.jsonl\", 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "      json_line = json.loads(line)\n",
        "      l.append(DocumentElement(**json_line))\n",
        "\n",
        "# Definimos una estructura que contenga nuestros elementos atómicos\n",
        "class Chunk(BaseModel):\n",
        "    document_id: str          # Identificador del documento al que pertenece.\n",
        "    label: str                # Etiqueta específica del modelo (ej: 'text', 'title', 'table').\n",
        "    region_label: str         # Etiqueta de la región a la que pertenece (ej: 'header', 'body').\n",
        "    embedding: List[float]    # Para construir el modelo, usamos texto o imagen según convenga\n",
        "    content: str              # Usamos base64 en caso sea imagen\n",
        "\n",
        "# Cargamos el modelo\n",
        "model = SentenceTransformer(\n",
        "    'jinaai/jina-clip-v2', trust_remote_code=True, truncate_dim=truncate_dim\n",
        ")"
      ],
      "metadata": {
        "id": "iL8X_8ZkGO51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es ahora cuando definimos la estrategia para crear nuestros `chunks` de datos:"
      ],
      "metadata": {
        "id": "PUm91l66NbhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "text = ''\n",
        "for elem in l:\n",
        "    if elem.label in ['image', 'chart']:\n",
        "        # Codificamos la imagen\n",
        "        img = Image.fromarray(np.array(elem.content, dtype=np.uint8))\n",
        "        emb = model.encode(img, normalize_embeddings=True)\n",
        "        # Convertimos la imagen a base64 (la forma en la que los LLMs consumen imágenes)\n",
        "        buffered = io.BytesIO()\n",
        "        img.save(buffered, format=format)\n",
        "        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "        mime_type = f\"image/{format.lower()}\"\n",
        "        base64_image = f\"data:{mime_type};base64,{img_str}\"\n",
        "        # Guardamos un chunk independiente al flujo de texto\n",
        "        chunks.append(\n",
        "            Chunk(\n",
        "                document_id=elem.document_id,\n",
        "                label=elem.label,\n",
        "                region_label=elem.region_label,\n",
        "                embedding=emb.tolist(),\n",
        "                content=base64_image,\n",
        "            )\n",
        "        )\n",
        "    elif elem.label in ['text', 'paragraph_title', 'content']:\n",
        "        tokenized_text_len = len(model.tokenizer.encode(text))\n",
        "        # Si el texto tiene una longitud menor a la que contemplamos como posible\n",
        "        if tokenized_text_len<min_chunk_size:\n",
        "            text += ' ' + elem.content\n",
        "        # Si el texto se encuentra en una longitud candidata para ser un chunk\n",
        "        elif ((tokenized_text_len>=min_chunk_size) & (tokenized_text_len<=max_chunk_size)):\n",
        "            embs = model.encode([text, elem.content], normalize_embeddings=True)\n",
        "            # Si la similitud es demasiado cercana\n",
        "            if float(np.dot(embs[0], embs[1]))>=similarity_threshold:\n",
        "                text += ' ' + elem.content\n",
        "            # Si los textos no se parecen lo suficiente\n",
        "            else:\n",
        "                # Guardamos el texto del chunk hasta la fecha\n",
        "                chunks.append(\n",
        "                    Chunk(\n",
        "                        document_id=elem.document_id,\n",
        "                        label='text',\n",
        "                        region_label='text',\n",
        "                        embedding=embs[0].tolist(),\n",
        "                        content=text,\n",
        "                    )\n",
        "                )\n",
        "                # Empezamos nuevo chunk\n",
        "                text = elem.content\n",
        "        # Si el texto ya excede la longitud establecida\n",
        "        else:\n",
        "            # Guardamos el texto del chunk hasta la fecha\n",
        "            emb = model.encode(text, normalize_embeddings=True)\n",
        "            chunks.append(\n",
        "                Chunk(\n",
        "                    document_id=elem.document_id,\n",
        "                    label='text',\n",
        "                    region_label='text',\n",
        "                    embedding=emb.tolist(),\n",
        "                    content=text,\n",
        "                )\n",
        "            )\n",
        "            # Empezamos nuevo chunk\n",
        "            text = elem.content"
      ],
      "metadata": {
        "id": "7DM7UHFHpzaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 5: Indexación en base de datos\n",
        "\n",
        "Finalmente, emplearemos un tipo especial de base de datos, que se ha popularizado mucho con los recientea avances de IA generativa, que son las **bases de datos de vectores**. Éstas implementan algoritmos específicos para búsqueda eficiente de información que viene almacenada en el formato que hemos preparado."
      ],
      "metadata": {
        "id": "IdD7mNf-NqKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencias\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "\n",
        "# Definimos un cliente local en memoria\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "# Creamos una nueva colección para albergar nuestros documentos\n",
        "client.create_collection(\n",
        "    collection_name=f\"{fn}_collection\",\n",
        "    vectors_config=VectorParams(size=truncate_dim, distance=Distance.COSINE),\n",
        ")"
      ],
      "metadata": {
        "id": "bUmGlktPN3tu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}